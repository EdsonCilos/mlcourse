{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mp_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Professor Edson Cilos\n",
        "\n",
        "Linkedin: https://www.linkedin.com/in/edson-cilos-032a66162/\n",
        "\n",
        "Website: https://edsoncilos.com\n",
        "\n",
        "Esse material é explicado com detalhes no meu [curso](https://www.udemy.com/course/edson-cilos-ml/?referralCode=2C9C581FAB301BBAE173)."
      ],
      "metadata": {
        "id": "13aWOK_9do_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análise de microplásticos no oceano**\n",
        "\n",
        "---\n",
        "\n",
        "Nesta atividade vamos treinar um modelo para caraterização de microplásticos no oceanos"
      ],
      "metadata": {
        "id": "JvksKrVsMNHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar algumas das bibliotecas importantes\n",
        "\n",
        "#sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#Computação científica\n",
        "from scipy.sparse.linalg import spsolve\n",
        "from scipy import sparse\n",
        "\n",
        "# pandas e numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Download arquivos\n",
        "from six.moves import urllib\n",
        "\n",
        "seed = 10"
      ],
      "metadata": {
        "id": "2wFb3JdENyDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = \"https://raw.githubusercontent.com/EdsonCilos/mp_classification/\\\n",
        "master/data/d4_rebuild.csv\"\n",
        "\n",
        "save_path = \"data.csv\"\n",
        "\n",
        "def save_file_from_url(link = link, save_path=save_path):\n",
        "    urllib.request.urlretrieve(link, save_path)"
      ],
      "metadata": {
        "id": "YszWUCIgMxuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_file_from_url()"
      ],
      "metadata": {
        "id": "IodTP2s-Nq7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(save_path)"
      ],
      "metadata": {
        "id": "W0MeZmmXNta2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conhecendo os dados"
      ],
      "metadata": {
        "id": "O8rrnb6xQzm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "VW_I1cbkQ2HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na nossa aplicação a coluna \"Nom\" não tem aplicação para fins preditivos"
      ],
      "metadata": {
        "id": "t9cPhuqVRI8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['Nom '], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "BZoEEFo7ROjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos agora renomear os nosso rótulos (opcional)"
      ],
      "metadata": {
        "id": "wrqPGT3ORVj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns={'Interpretation ': \"label\"}, inplace=True)"
      ],
      "metadata": {
        "id": "WQxaZuNARPmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos estudar a distribuição dos rótulos"
      ],
      "metadata": {
        "id": "59ltxV_zRlLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "data[\"label\"].hist(bins=50, figsize=(30,15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xYuFFTzTRbd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "KcIrJfIrRfnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparando os dados"
      ],
      "metadata": {
        "id": "207PS_MbUxT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos verificar a presença de dados faltantes"
      ],
      "metadata": {
        "id": "mpL6hhI0SNml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(data[data.isnull().any(axis=1)])"
      ],
      "metadata": {
        "id": "WUlDFEBIR12q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos incluir todas as classes pouco representativas em Unknown"
      ],
      "metadata": {
        "id": "Wr8gyfAdSYak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_less_representative(data, threshold = 5):\n",
        "\n",
        "  dataset = data.copy()\n",
        "\n",
        "  freq = dataset[\"label\"].value_counts()\n",
        "  less_rep = [idx for idx, value in freq.items()  if value < threshold]\n",
        "  \n",
        "  for i, row in dataset.iterrows():\n",
        "      if row[\"label\"] in less_rep:\n",
        "          dataset.at[i, 'label'] = 'Unknown'\n",
        "\n",
        "  return dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "AMAMKewcSJe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = transform_less_representative(data)"
      ],
      "metadata": {
        "id": "8bm-tVtGSmpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "v0GaEjKpTEPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos conferir rapidinho os tipos de dados"
      ],
      "metadata": {
        "id": "al8wpykMVkva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "WIj0n3S5ViSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes.unique()"
      ],
      "metadata": {
        "id": "_F_r5TVDVTl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos agora codificar os rótulos do problema"
      ],
      "metadata": {
        "id": "P4sZGgAIVM7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "df[\"label\"] = encoder.fit_transform(df[\"label\"])"
      ],
      "metadata": {
        "id": "2n9Nm8P2TGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aproveitar e revisar um pouco sobre os codificadores"
      ],
      "metadata": {
        "id": "JrcQyf1jV0N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].unique()"
      ],
      "metadata": {
        "id": "yWD3Mk2wVusG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.inverse_transform(range(14))"
      ],
      "metadata": {
        "id": "dNZirrhHVwSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.inverse_transform([0])"
      ],
      "metadata": {
        "id": "A7jtLFrlWx4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.inverse_transform([13])"
      ],
      "metadata": {
        "id": "5CFLAv3LW-UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.inverse_transform([1, 1, 7, 3, 3, 8])"
      ],
      "metadata": {
        "id": "BhW4vx8FW_tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos agora criar a matriz de características e o vetor de rótulos"
      ],
      "metadata": {
        "id": "wUptHjwGXLbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop([\"label\"], axis = 1)\n",
        "y = df[\"label\"].copy()"
      ],
      "metadata": {
        "id": "ZCtWs4qVXQ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separando em treino e teste"
      ],
      "metadata": {
        "id": "l9tvQ44MXYd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    stratify=y, \n",
        "                                                    test_size=1/3,\n",
        "                                                    random_state = seed)"
      ],
      "metadata": {
        "id": "F7HMhVTXXRZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinando primeiro modelo"
      ],
      "metadata": {
        "id": "MYTBrnQWYo9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos começar treinando um SVM com kernel linear. Faremos isso usando de diferentes formas:\n",
        "\n",
        "\n",
        "\n",
        "*   Sem escalonamento de atributos\n",
        "*   Com escalonamento de atributos, de maneira rigorosa\n",
        "*   Com escalonamento de atributos, de forma enviesada\n"
      ],
      "metadata": {
        "id": "GGJAz_QZYvX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classificador base\n",
        "clf = SVC(kernel='linear', C=0.0015, probability=True)\n",
        "\n",
        "scoring = {'acc': 'accuracy', \n",
        "           'neg_loss': 'neg_log_loss'}"
      ],
      "metadata": {
        "id": "EdyRuM0tis8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sem escalonamento\n",
        "score = cross_validate(clf, X_train, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "Zds2sbf9fxoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline rigoroso\n",
        "pipe = Pipeline([\n",
        "                 ('std_scaler', StandardScaler()),\n",
        "                 ('classifier', clf)\n",
        "                 ])\n",
        "\n",
        "score = cross_validate(pipe, X_train, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "c68ZzHySYqU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metodologia enviesada\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "score = cross_validate(clf, X_scaled, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")\n"
      ],
      "metadata": {
        "id": "RlnIIyfKg1SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consulte os especialistas!"
      ],
      "metadata": {
        "id": "72W9k4LAj8_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "É comum fazer a \"correção da linha de base\" dentro de técnicas de FTIR (espectroscopia de infravermelho por transformada de Fourier)"
      ],
      "metadata": {
        "id": "a3tHhkehlkcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def als(y, lam=1e5, p=0.01, itermax=10):\n",
        "    \"\"\"\n",
        "    Implements an Asymmetric Least Squares Smoothing\n",
        "    baseline correction algorithm (P. Eilers, H. Boelens 2005)\n",
        "\n",
        "    Baseline Correction with Asymmetric Least Squares Smoothing\n",
        "    based on https://github.com/vicngtor/BaySpecPlots\n",
        "\n",
        "    Baseline Correction with Asymmetric Least Squares Smoothing\n",
        "    Paul H. C. Eilers and Hans F.M. Boelens\n",
        "    October 21, 2005\n",
        "\n",
        "    Description from the original documentation:\n",
        "\n",
        "    Most baseline problems in instrumental methods are characterized by a\n",
        "    smooth baseline and a superimposed signal that carries the analytical \n",
        "    information: a series of peaks that are either all positive or all \n",
        "    negative. We combine a smoother with asymmetric weighting of deviations \n",
        "    from the (smooth) trend get an effective baseline estimator. \n",
        "    It is easy to use, fast and keeps the analytical peak signal intact.\n",
        "    No prior information about peak shapes or baseline (polynomial) is needed\n",
        "    by the method. The performance is illustrated by simulation and \n",
        "    applications to real data.\n",
        "\n",
        "    Inputs:\n",
        "        y:\n",
        "            input data (i.e. chromatogram of spectrum)\n",
        "        lam:\n",
        "            parameter that can be adjusted by user. The larger lambda is,\n",
        "            the smoother the resulting background, z\n",
        "        p:\n",
        "            wheighting deviations. 0.5 = symmetric, <0.5: negative\n",
        "            deviations are stronger suppressed\n",
        "        itermax:\n",
        "            number of iterations to perform\n",
        "    Output:\n",
        "        the fitted background vector\n",
        "\n",
        "    \"\"\"\n",
        "    L = len(y)\n",
        "    D = sparse.eye(L, format='csc')\n",
        "    D = D[1:] - D[:-1]  \n",
        "    D = D[1:] - D[:-1]\n",
        "    D = D.T\n",
        "    w = np.ones(L)\n",
        "    for i in range(itermax):\n",
        "        W = sparse.diags(w, 0, shape=(L, L))\n",
        "        Z = W + lam * D.dot(D.T)\n",
        "        z = spsolve(Z, w * y)\n",
        "        w = p * (y > z) + (1 - p) * (y < z)\n",
        "    return z"
      ],
      "metadata": {
        "id": "-IgT32-spdHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample(sample, sample2 = None, baseline=True):\n",
        "\n",
        "    horizontal = [int(x.split('.')[0]) for x in sample.index.values]\n",
        "\n",
        "    values = sample.values\n",
        "\n",
        "    plt.xlabel(\"Wavelenght (1/cm)\")\n",
        "\n",
        "    plt.plot(horizontal, values, color = (1/255,209/255,209/255), \n",
        "             label = 'sample')\n",
        "    \n",
        "    if sample2 is not None:\n",
        "      plt.plot(horizontal, sample2.values, color = 'red', label = 'sample 2')\n",
        "    \n",
        "    if baseline:\n",
        "      plt.plot(horizontal, values - als(values), \n",
        "              color = (90/255, 53/255, 182/255), \n",
        "              label= 'sample (corrected)')\n",
        "             \n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "VqirFhq4eHAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(X_train.iloc[0])"
      ],
      "metadata": {
        "id": "z6KVRVTCpsNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos agora aplicar a nossa correção de base para todos os dados:\n",
        "\n",
        "**OBS:** A correção de cada instância não usa informação do restante dos dados, de forma que a correção pode ser feita em todos os dados sem introduzir viés / data leakage"
      ],
      "metadata": {
        "id": "te6pmucH0yOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Aplicando correção de linha de base...\")\n",
        "\n",
        "old_sample = X_train.iloc[0].copy()\n",
        "\n",
        "\n",
        "print(\"Transformando conjunto de treino\")\n",
        "for idx, row in X_train.iterrows():\n",
        "  X_train.loc[idx, :] = row - als(row)\n",
        "\n",
        "\n",
        "print(\"Transformando conjunto de teste\")\n",
        "for idx, row in X_test.iterrows():\n",
        "  X_test.loc[idx, :] = row - als(row)\n",
        "\n",
        "print(\"Processo finalizado!\")"
      ],
      "metadata": {
        "id": "RP-F8mjBpu6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(old_sample, baseline=False)"
      ],
      "metadata": {
        "id": "BzJowh7o3GDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(X_train.iloc[0], baseline=False)"
      ],
      "metadata": {
        "id": "qMPhyN3000xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos repetir a etapa de escalonamento:"
      ],
      "metadata": {
        "id": "GQzusACX6Aa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sem escalonamento\n",
        "score = cross_validate(clf, X_train, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "bNbzgD_71ztY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline rigoroso\n",
        "pipe = Pipeline([\n",
        "                 ('std_scaler', StandardScaler()),\n",
        "                 ('classifier', clf)\n",
        "                 ])\n",
        "\n",
        "score = cross_validate(pipe, X_train, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "GB3yUNQx6DHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ganhamos 7% só ao usar a metodologia correta para preparar os dados!"
      ],
      "metadata": {
        "id": "dGkxe7dp6SWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metodologia enviesada\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "score = cross_validate(clf, X_scaled, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "6lDt1WQ26Hk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A metologia errada continua aumentando a performance do modelo, ainda que marginal."
      ],
      "metadata": {
        "id": "jmfv1ybD6amG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de componentes principais"
      ],
      "metadata": {
        "id": "XIf4JgZn9aS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aplicar a análise de componentes principais para redução de dimensionalidade"
      ],
      "metadata": {
        "id": "bcF7KJxX9fmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline rigoroso\n",
        "pipe = Pipeline([\n",
        "                 ('std_scaler', StandardScaler()),\n",
        "                 (\"dim_red\", PCA(n_components=100, random_state=seed)),\n",
        "                 ('classifier', clf)\n",
        "                 ])\n",
        "\n",
        "score = cross_validate(pipe, X_train, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "dG5vQ1ekIN_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metodologia enviesada\n",
        "scaler = StandardScaler()\n",
        "n_components= 100\n",
        "pca = PCA(n_components= n_components, random_state=seed)\n",
        "\n",
        "X_transformed = pca.fit_transform(scaler.fit_transform(X_train))\n",
        "\n",
        "score = cross_validate(clf, X_transformed, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "bubhXAeuI7HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos por enquanto esquecer a validação cruzada e explorar o conjunto de dados transformado via escalonamento + std scalar"
      ],
      "metadata": {
        "id": "sv0uNnhmJ50i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "7dzL_Qh8K3Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organizando a informação"
      ],
      "metadata": {
        "id": "QYEjHgDdK4QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explained_ratio = np.round(np.sum(pca.explained_variance_ratio_)*100, 3)\n",
        "\n",
        "print(\"Com {} componentes: {} % da variância explicada\".format(\n",
        "    n_components, explained_ratio))\n"
      ],
      "metadata": {
        "id": "3bvCptiJKDKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos ver a contribuição acumulada:"
      ],
      "metadata": {
        "id": "7vhtBIHaLGR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(np.cumsum(pca.explained_variance_ratio_)*100, 3)"
      ],
      "metadata": {
        "id": "F0OgF8hFLEw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos determinar o número mínimo de componentes para explicar 99% das característica:"
      ],
      "metadata": {
        "id": "_j1Z7CMsLZdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components= n_components, random_state=seed)\n",
        "scaler = StandardScaler()\n",
        "X_transformed = pca.fit_transform(scaler.fit_transform(X_train))\n",
        "cum_sum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cum_sum >= 0.99) + 1\n",
        "print(d)"
      ],
      "metadata": {
        "id": "W7F-M2fNKFHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porém existe uma maneira bem mais direta de fazer isto:"
      ],
      "metadata": {
        "id": "IjIeKbJ-MpDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components= 0.99, random_state=seed)\n",
        "scaler = StandardScaler()\n",
        "X_transformed = pca.fit_transform(scaler.fit_transform(X_train))\n",
        "print(np.round(np.sum(pca.explained_variance_ratio_)*100, 3))"
      ],
      "metadata": {
        "id": "bDP7geLlMhoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.n_components_"
      ],
      "metadata": {
        "id": "56XAMky_M5Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dimension_vs_explained_variance(X_data):\n",
        "\n",
        "    pca = PCA(n_components= 100, random_state=seed)\n",
        "    scaler = StandardScaler()\n",
        "    X_transformed = pca.fit_transform(scaler.fit_transform(X_data))\n",
        "\n",
        "    cum_sum = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "    plt.xlabel(\"Dimensões\")\n",
        "    plt.ylabel(\"Variância explicada\")\n",
        "\n",
        "    plt.plot(range(len(cum_sum)), \n",
        "             cum_sum, \n",
        "             color = (1/255,209/255,209/255))\n",
        "\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "xOTAvKZRNgNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dimension_vs_explained_variance(X_train)"
      ],
      "metadata": {
        "id": "8JY5OfOTPFjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components= 25, random_state=seed)\n",
        "scaler = StandardScaler()\n",
        "X_transformed = pca.fit_transform(scaler.fit_transform(X_train))\n",
        "print(np.round(np.sum(pca.explained_variance_ratio_)*100, 3))"
      ],
      "metadata": {
        "id": "JODX8fPPPNEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA para compressão"
      ],
      "metadata": {
        "id": "1SwIdcVMPzq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos revisar a nossa amostra de exemplo:"
      ],
      "metadata": {
        "id": "tFzPwiDuQjkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(X_train.iloc[0], baseline=False)"
      ],
      "metadata": {
        "id": "JGo0CtaTPrYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components= 46, random_state=seed)\n",
        "scaler = StandardScaler()\n",
        "X_transformed = pca.fit_transform(scaler.fit_transform(X_train))"
      ],
      "metadata": {
        "id": "TRfACuKbQqmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_recovered = pd.DataFrame(\n",
        "    data = scaler.inverse_transform(pca.inverse_transform(X_transformed)),\n",
        "    columns = X_train.columns\n",
        ")"
      ],
      "metadata": {
        "id": "Sow3MIdiUaM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sample(sample = X_train.iloc[0],  \n",
        "            sample2 = X_recovered.iloc[0],\n",
        "            baseline=False)"
      ],
      "metadata": {
        "id": "TBzygMLCS4kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "eJ-3dWzmVCFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data = X_transformed).info()"
      ],
      "metadata": {
        "id": "UZmfv4dgUqbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Será que vale a pena essa compressão?"
      ],
      "metadata": {
        "id": "mnEeLEU8WWMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline rigoroso\n",
        "pipe = Pipeline([\n",
        "                 ('std_scaler', StandardScaler()),\n",
        "                 (\"dim_red\", PCA(n_components=46, random_state=seed)),\n",
        "                 ('classifier', clf)\n",
        "                 ])\n",
        "\n",
        "score = cross_validate(pipe, X_train, y_train, \n",
        "                        scoring=scoring, cv=5)\n",
        "\n",
        "print(\"Log loss: \" + str(np.round(-np.mean(score['test_neg_loss']), 3)))\n",
        "print(\"Acurácia: \" + str(np.round(100*np.mean(score['test_acc']), 2)) + \"%\")"
      ],
      "metadata": {
        "id": "N6YHQB4EVDtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação final"
      ],
      "metadata": {
        "id": "oarpy5_MZY1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "                 ('std_scaler', StandardScaler()),\n",
        "                 (\"dim_red\", PCA(n_components=46, random_state=seed)),\n",
        "                 ('classifier', clf)\n",
        "                 ])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "pSrHINiGZsJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_loss(y_test, model.predict_proba(X_test))"
      ],
      "metadata": {
        "id": "gNhRMy4eZ9t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(np.array(y_test), model.predict(X_test))"
      ],
      "metadata": {
        "id": "ALtLvPzeaAKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(encoder, open('encoder.sav', 'wb'))\n",
        "pickle.dump(model, open('classifier.sav', 'wb'))"
      ],
      "metadata": {
        "id": "8sCvs_DGaBlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus"
      ],
      "metadata": {
        "id": "RC8teoHXdBPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "link_sample = \"https://raw.githubusercontent.com/EdsonCilos/mlcourse/master/\\\n",
        "reducao_dimensionalidade/PCA/prediction_132.png\"\n",
        "\n",
        "response = requests.get(link_sample)\n",
        "img = Image.open(BytesIO(response.content))"
      ],
      "metadata": {
        "id": "e5C4s939jN8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "Y3ub5OdMjXaY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}